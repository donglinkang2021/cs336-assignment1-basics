### 7.3 Ablations and architecture modification

The best way to understand the Transformer is to actually modify it and see how it behaves. We will now do a few simple ablations and modifications.

#### Ablation 1: layer normalization

It is often said that layer normalization is important for the stability of Transformer training. But perhaps we want to live dangerously. Let’s remove `RMSNorm` from each of our Transformer blocks and see what happens.

**Problem (layer_norm_ablation): Remove `RMSNorm` and train (1 point) (1 H100 hr)**

Remove all of the `RMSNorm`s from your Transformer and train. What happens at the previous optimal learning rate? Can you get stability by using a lower learning rate?

*   **Deliverable**: A learning curve for when you remove `RMSNorm`s and train, as well as a learning curve for the best learning rate.
*   **Deliverable**: A few sentence commentary on the impact of `RMSNorm`.

Let’s now investigate another layer normalization choice that seems arbitrary at first glance. Pre-norm Transformer blocks are defined as

```
z = x + MultiHeadedSelfAttention(RMSNorm(x))
y = z + FFN(RMSNorm(z))
```

This is one of the few ‘consensus’ modifications to the original Transformer architecture, which used a post-norm approach as

```
z = RMSNorm(x + MultiHeadedSelfAttention(x))
y = RMSNorm(z + FFN(z))
```

Let’s revert back to the post-norm approach and see what happens.

**Problem (pre_norm_ablation): Implement post-norm and train (1 point) (1 H100 hr)**

Modify your pre-norm Transformer implementation into a post-norm one. Train with the post-norm model and see what happens.

*   **Deliverable**: A learning curve for a post-norm transformer, compared to the pre-norm one.

We see that layer normalization has a major impact on the behavior of the transformer, and that even the position of the layer normalization is important.

#### Ablation 2: position embeddings

We will next investigate the impact of the position embeddings on the performance of the model. Specifically, we will compare our base model (with RoPE) with not including position embeddings at all (NoPE). It turns out that decoder-only transformers, i.e., those with a causal mask as we have implemented, can in theory infer relative or absolute position information without being provided with position embeddings explicitly [Tsai et al., 2019, Kazemnejad et al., 2023]. We will now test empirically how NoPE performs compare to RoPE.

**Problem (no_pos_emb): Implement NoPE (1 point) (1 H100 hr)**

Modify your Transformer implementation with RoPE to remove the position embedding information entirely, and see what happens.

*   **Deliverable**: A learning curve comparing the performance of RoPE and NoPE.

#### Ablation 3: SwiGLU vs. SiLU

Next, we will follow Shazeer [2020] and test the importance of gating in the feed-forward network, by comparing the performance of SwiGLU feed-forward networks versus feed forward networks using SiLU activations but no gated linear unit (GLU):

```
FFNSiLU(x) = W2 * SiLU(W1 * x)
```

Recall that in our SwiGLU implementation, we set the dimensionality of the inner feed-forward layer to be roughly `d_ff = 8/3 * d_model` (while ensuring that `d_ff mod 64 = 0`, to make use of GPU tensor cores). In your `FFNSiLU` implementation you should set `d_ff = 4 * d_model`, to approximately match the parameter count of the SwiGLU feed-forward network (which has three instead of two weight matrices).

**Problem (swiglu_ablation): SwiGLU vs. SiLU (1 point) (1 H100 hr)**

*   **Deliverable**: A learning curve comparing the performance of SwiGLU and SiLU feed-forward networks, with approximately matched parameter counts.
*   **Deliverable**: A few sentences discussing your findings.

> **Low-Resource/Downscaling Tip: Online students with limited GPU resources should test modifications on TinyStories**
>
> In the remainder of the assignment, we will move to a larger-scale, noisier web dataset (OpenWebText), experimenting with architecture modifications and (optionally) making a submission to the course leaderboard.
>
> It takes a long time to train an LM to fluency on OpenWebText, so we suggest that online students with limited GPU access continue testing modifications on TinyStories (using validation loss as a metric to evaluate performance).