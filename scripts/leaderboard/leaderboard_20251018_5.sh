export WANDB_MODE=offline
CUDA_VISIBLE_DEVICES=0 uv run train_muon.py \
    +model.add_qknorm=True \
    model.vocab_size=32000 \
    model.num_layers=12 \
    model.num_heads=12 \
    model.d_model=768 \
    model.d_ff=2048 \
    data.path=data/openwebtext-32k \
    data.tokenizer_path=hf_tokenizer/openwebtext-32k/tokenizer.json \
    training.batch_size=128 \
    training.max_iters=20000 \
    optimizer.max_lr=1e-2 \
    optimizer.min_lr=0 \
    optimizer.weight_decay=0.01 \
    optimizer.max_l2_norm=2.0 \
    optimizer.betas="[0.95,0.999]" \
    'logger.run_name=leaderboard-qknorm-muon-betas1-gpt2'

CUDA_VISIBLE_DEVICES=0 uv run train_muon.py \
    +model.add_qknorm=True \
    model.vocab_size=32000 \
    model.num_layers=12 \
    model.num_heads=12 \
    model.d_model=768 \
    model.d_ff=2048 \
    data.path=data/openwebtext-32k \
    data.tokenizer_path=hf_tokenizer/openwebtext-32k/tokenizer.json \
    training.batch_size=128 \
    training.max_iters=20000 \
    optimizer.max_lr=1e-3 \
    optimizer.min_lr=0 \
    optimizer.weight_decay=0.01 \
    optimizer.max_l2_norm=2.0 \
    optimizer.betas="[0.95,0.999]" \
    'logger.run_name=leaderboard-qknorm-muon-betas1-gpt2-lr1e-3'

CUDA_VISIBLE_DEVICES=0 uv run train.py \
    +model.add_qknorm=True \
    model.vocab_size=32000 \
    model.num_layers=12 \
    model.num_heads=12 \
    model.d_model=768 \
    model.d_ff=2048 \
    data.path=data/openwebtext-32k \
    data.tokenizer_path=hf_tokenizer/openwebtext-32k/tokenizer.json \
    training.batch_size=128 \
    training.max_iters=20000 \
    optimizer.max_lr=1e-2 \
    optimizer.min_lr=0 \
    optimizer.weight_decay=0.01 \
    optimizer.max_l2_norm=2.0 \
    optimizer.betas="[0.95,0.999]" \
    'logger.run_name=leaderboard-qknorm-betas1-gpt2'

CUDA_VISIBLE_DEVICES=0 uv run train.py \
    +model.add_qknorm=True \
    model.vocab_size=32000 \
    model.num_layers=12 \
    model.num_heads=12 \
    model.d_model=768 \
    model.d_ff=2048 \
    data.path=data/openwebtext-32k \
    data.tokenizer_path=hf_tokenizer/openwebtext-32k/tokenizer.json \
    training.batch_size=128 \
    training.max_iters=20000 \
    optimizer.max_lr=1e-3 \
    optimizer.min_lr=0 \
    optimizer.weight_decay=0.01 \
    optimizer.max_l2_norm=2.0 \
    optimizer.betas="[0.95,0.999]" \
    'logger.run_name=leaderboard-qknorm-betas1-gpt2-lr1e-3'